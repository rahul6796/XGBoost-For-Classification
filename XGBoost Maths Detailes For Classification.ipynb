{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Maths Detailes For Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are consider the simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## draw the data as an example which we are consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Values For  a Leaf node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $x-axis$ contains the $Drug$ $Dosage$ and $y-axis$ contains the Probability that the Drug is effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if are using $XGBoost$ for $Classification$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then this, the negative log-likelihood is the most commonly used $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y_{i},p_{i}) = -[y_{i}\\log(p_{i}) + (1- y_{i})\\log(1 -p_{i})]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the derivative took a long time because the output values are in terms of the $log(odds)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we converted the Probability $(p_{i})$ to $log(odds)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y_{i},log(odds)_{i}) = -y_{i}\\log(odds) + \\log(1 + e^{\\log(odds)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we took the derivative of this $Loss$ $Function$ with respect to $log(odds)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial }{\\partial \\log(odds)}L(y_{i},\\log(odds)_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " =  - y_{i} + \\frac{e^{\\log(odds)_{i}}}{1 + e^{\\log(odds)_{i}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we know in advanced this term is equal to..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{e^{\\log(odds)_{i}}}{1 + e^{\\log(odds)_{i}}} = p_{i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial }{\\partial \\log(odds)}L(y_{i},\\log(odds)_{i}) = -(y_{i} - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we take second derivative of the this $Loss$ $Function$ then  we got.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial^{2}}{\\partial \\log(odds)^{2}}L(y_{i},log(odds)_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\log(odds)}[\\frac{\\partial}{\\partial \\log(odds)} L(y_{i},\\log(odds)_{i})]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\log(odds)}(-y_{i} +  \\frac{e^{\\log(odds)_{i}}}{1 + e^{\\log(odds)_{i}}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial^{2}}{\\partial \\log(odds)^{2}}L(y_{i},log(odds)_{i}) = \\frac{e^{\\log(odds)}}{1 + e^{\\log(odds)}} *\\frac{1}{1 + e^{\\log(odds)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{\\partial^{2}}{\\partial \\log(odds)^{2}}L(y_{i},log(odds)_{i}) = p_{i}*(1 - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the first derivative of the $Loss$ $Function$ the $Gradient$ ,$g_{i}$.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g_{i} = \\frac{\\partial }{\\partial \\log(odds)}L(y_{i},\\log(odds)_{i}) = -(y_{i} - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the second derivative of the $Loss$ $Function$ , the $Hessian$, $h_{i}$.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h_{i} = \\frac{\\partial^{2}}{\\partial \\log(odds)^{2}}L(y_{i},log(odds)_{i}) = p_{i}*(1 - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plug them into the  equation for the optimal $Output$ $Value$ $(O_{value})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{-(g_{1}+g_{2}+....+g_{n})}{(h_{1}+h_{1}+.....+h_{1} +{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just like for $Regression$ ,$g_{i}$ is the negative $Residual$, so we can replace the numerator  for the $O_{value}$ with the sum of the residual.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{-(-(y_{1}-p_{1}) - (y_{2}-p_{2}) + ....-(y_{n} - p_{n}))}{(h_{1}+h_{1}+.....+h_{1} +{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{(y_{1}-p_{1}) + (y_{2}-p_{2}) + ....+(y_{n} - p_{n})}{(h_{1}+h_{1}+.....+h_{1} +{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{(\\sum Residual_{i})}{(h_{1}+h_{1}+.....+h_{1} +{\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the denominator we can just replace all of the $h_{i}$ with the sum of $p_{i}*(1 - p_{i})$... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{(\\sum Residual_{i})}{\\sum [Prevoius Probability_{i}*(1 - Previous Probability_{i})] + {\\lambda}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Note:-$ In the denominator we are using $Previous$ $Probability$ to specify to the previously predicted probability rather than the previously predicted $log(odds)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we using $XGBoost$ for $Classification$ this is the specific formula for the $Output$ $Value$ for a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{(\\sum Residual_{i})}{\\sum [Prevoius Probability_{i}*(1 - Previous Probability_{i})] + {\\lambda}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now regardless of whether  we are using $XGBoost$ for $Regression$ or $Classification$.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the optimal $Output$ $Value$ for this leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we do it by plugging derivative of the $Loss$ $Function$ into equation for the $Output$ $Value$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "O_{value} = \\frac{(\\sum Residual_{i})}{\\sum [Prevoius Probability_{i}*(1 - Previous Probability_{i})] + {\\lambda}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to derive the equations for the $Similarity$ $Scores$ so we can grow the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Score For Growing  The Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, before we do that, remember that we derived the equation for the $O_{value}$ by minimizing  the sum of the $Loss$ $Function$ plus the $Regularization$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p^{o}_{i} + O_{value}) + \\frac{1}{2}{\\lambda}O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's also remember that depending on the $Loss$ $Function$ , optimizing this part can be hard =, so we approximate it with a $Second$ $Order$ $Taylor$ $Polynomial$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "L(y_{i},p_{i} + O_{value}) \\approx L(y,p_{i}) +g*O_{value} + \\frac{1}{2}*h*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we expanded the summation, and added the $Regularization$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p^{o}_{i} + O_{value}) \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "=L(y_{1},p^{o}_{1} + O_{value})+L(y_{2},p^{o}_{2} + O_{value})+.....+L(y_{n},p^{o}_{n} + O_{value}) + \\frac{1}{2}{\\lambda}*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and swapped in the $Second$ $Order$ $Taylor$ $Polynomial$ of the $Loss$ $Function$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= L(y_{1},p_{1}^{0}) +g_{1}*O_{value} + \\frac{1}{2}*h_{1}*O_{value}^{2}+L(y_{2},p_{2}^{0}) +g_{2}*O_{value} + \\frac{1}{2}*h_{2}*O_{value}^{2} +...+L(y_{n},p_{n}^{0}) +g_{n}*O_{value} + \\frac{1}{2}*h_{n}*O_{value}^{2} + \\frac{1}{2}{\\lambda}*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we remove the constant term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= g_{1}*O_{value} + \\frac{1}{2}*h_{1}*O_{value}^{2}+g_{2}*O_{value} + \\frac{1}{2}*h_{2}*O_{value}^{2} +...+g_{n}*O_{value} + \\frac{1}{2}*h_{n}*O_{value}^{2} + \\frac{1}{2}{\\lambda}*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we simplify this above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "= (g_{1} + g_{2}+...+g_{n})*O_{value} + \\frac{1}{2}(h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ,because we removed constant terms when deriving this equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's not equalt to what we started with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}L(y_{i},p^{o}_{i} + O_{value}) + \\frac{1}{2}{\\lambda}O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we plotted both the equations on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot this equation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we did see that the same $x-axis$ coordinate represented by the $O_{value}$ tells us the location of the lowest point in the both parabolas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " (g_{1} + g_{2}+...+g_{n})*O_{value} + \\frac{1}{2}(h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So , I mention this because $XGBoost$ uses the simplified equation to determine the $Similarity$ $Score$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so , the first thing $XGBoost$ does is multiply everything by $-1$, that make each term negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " -[(g_{1} + g_{2}+...+g_{n})*O_{value} + \\frac{1}{2}(h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value}^{2}]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " -(g_{1} + g_{2}+...+g_{n})*O_{value} - \\frac{1}{2}(h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it flips  the parabola over the horizontal line $y$ = $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drwa the grapha..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the optimal $O_{value}$ represents the $x-axis$ coordinate for the highest point on the parabola.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this $y-axis$ coordinate for the highest point on the parabola is the $Similarity$ $Score$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the $Similarity$ $Score$ used in the implementations is actually $2$ times that number ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the graph which are showing the video lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for this difference will become clear once we do the algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so let's do the algebra to convert this into the $Similarity$ $Score$ we saw in $XGBoost$ for $Regression$ and $Classification$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first let's plug in the solution for the $O_{value}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " -(g_{1} + g_{2}+...+g_{n})*O_{value} - \\frac{1}{2}(h_{1}+h_{2}+...+h_{n}+{\\lambda})*O_{value}^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " -(g_{1} + g_{2}+...+g_{n})*\\frac{-(g_{1} + g_{2}+...+g_{n})}{(h_{1}+h_{2}+...+h_{n}+{\\lambda})} - \\frac{1}{2}(h_{1}+h_{2}+...+h_{n}+{\\lambda})*(\\frac{-(g_{1} + g_{2}+...+g_{n})}{(h_{1}+h_{2}+...+h_{n}+{\\lambda})})^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{(g_{1} + g_{2} +.....+g_{n})^{2}}{h_{1}+h_{2}+....+h_{n} + {\\lambda}} - \\frac{1}{2}*(\\frac{(g_{1}+g_{2} +....+{g}_{n})^{2}}{h_{1}+h_{2} + ....+h_{n} + {\\lambda}})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\frac{1}{2}*\\frac{(g_{1}+g_{2}+...+g_{n})^{2}}{(h_{1}+h_{2}+...+h_{n} + {\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the equation for $Similarity$ $Score$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Similarity Score = \\frac{1}{2}*\\frac{(g_{1}+g_{2}+...+g_{n})^{2}}{(h_{1}+h_{2}+...+h_{n} + {\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However in the $XGBoost$ implementation this $\\frac{1}{2}$ is omitted because the $Similarity$ $Score$ is only relative measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Similarity Score =\\frac{(g_{1}+g_{2}+...+g_{n})^{2}}{(h_{1}+h_{2}+...+h_{n} + {\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and  as long every $Similarity$ $Score$ is scaled the same amount the result of the comparisions will be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of the how $extreme$ e$X$treme $G$radient $B$oost is! if will do anything to reduce the amount of computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to derive the $Similarity$ $Score$ for $Classification$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the data points.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that just means plugging in the first derivative  of the $Loss$ $Function$, g_{i} and the second derivative $h_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "g_{i} = \\frac{\\partial }{\\partial \\log(odds)}L(y_{i},\\log(odds)_{i}) = -(y_{i} - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h_{i} = \\frac{\\partial^{2}}{\\partial \\log(odds)^{2}}L(y_{i},log(odds)_{i}) = p_{i}*(1 - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again , since g_{i} is the negative $Residual$, and the numerator is simply the $Sum$ $of$ $the$ $Residuals$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Similarity Score =\\frac{(\\sum Residual_{i})^{2}}{(h_{1}+h_{2}+...+h_{n} + {\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since $h_{i}$ is the previously predicted probability times one minus the previously predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then  the  denominator is just the sum of the $h_{i}$ plus ${\\lambda}$(lambda)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Similarity Score =\\frac{(\\sum Residual_{i})^{2}}{\\sum [Previous Probability_{i}*(1 -Previous Probability_{i} )] + {\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus this equation is the $Similarity$ $Score$ that the use for $Classification$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Cover$ is related to the Minimum number of $Residual$ in a leaf..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drwa a final tree in case of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we said that $Cover$ was the denominator of the $Similarity$ $Score$ minus ${\\lambda}$(lambda)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Similarity Score =\\frac{(g_{1}+g_{2}+...+g_{n})^{2}}{(h_{1}+h_{2}+...+h_{n} + {\\lambda})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words , $Cover$ is the sum of the $Hessian$, the $h_{i}'s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Cover = h_{1} +h_{2}+.....+h_{n} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $Classification$ the $Hessian$ is $p_{i}*(1 - p_{i})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "h_{i} = \\frac{\\partial^{2}}{\\partial \\log(odds)^{2}}L(y_{i},log(odds)_{i}) = p_{i}*(1 - p_{i})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so cover is equal to the sum of the previous predicted probability times one minus the previous predicted probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "Cover = h_{1} +h_{2}+.....+h_{n}  = \\sum [p_{i}*(1 - p_{i})]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
